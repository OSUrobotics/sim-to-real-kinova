{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9fd8e4e",
   "metadata": {},
   "source": [
    "# Real world replay buffer\n",
    "\n",
    "\n",
    "### Goal of this file: Make a transfer process to load our actions with the replay buffer given by DDPG algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa36a3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable jedi autocompleter\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "# reloads imported files. you're welcome\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# matplotlib inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f389e538",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb382018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data sci\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# generic packages\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "# our packages\n",
    "from utils import state_dim_setup, lerp\n",
    "from buffer import ReplayBuffer_Queue\n",
    "from DDPGfD import DDPGfD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c75cec0",
   "metadata": {},
   "source": [
    "### Constants / inputs\n",
    "\n",
    "1. real world experiment directory\n",
    "2. load successes only?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d2558e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_dir = 'real_positional_test_v6_constant'\n",
    "load_success_only = False\n",
    "state_idx_arr = state_dim_setup('adam_sim2real_v02')\n",
    "# state_idx_arr = state_dim_setup('adam_sim2real')  # TODO: yeet this shit once we get the new policy\n",
    "# trained_policy_path = 'policies/state_dim_full_train_v01/_07_22_21_0544/policy/train_DDPGfD_kinovaGrip'\n",
    "trained_policy_path = 'policies/state_dim_26_1000_eps/policy/train_DDPGfD_kinovaGrip'\n",
    "trained_policy_path = 'policies/policy_3500/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a144ccdd",
   "metadata": {},
   "source": [
    "### Loading replay buffer stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c04aedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add relative directory\n",
    "assert Path(real_dir).exists(), 'Check that the real log directory ' + real_dir + ' exists.'\n",
    "\n",
    "\n",
    "episode_dir = os.path.join(real_dir, 'episodes/')\n",
    "\n",
    "# get filepaths from episodes directory.\n",
    "reward_filepaths = sorted(glob.glob(os.path.join(episode_dir, 'reward*.npy')))\n",
    "obs_filepaths = sorted(glob.glob(os.path.join(episode_dir, 'obs*.npy')))\n",
    "next_obs_filepaths = sorted(glob.glob(os.path.join(episode_dir, 'next_obs*.npy')))\n",
    "action_filepaths = sorted(glob.glob(os.path.join(episode_dir, 'action*.npy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20d4f773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 30, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_arr = np.array([np.load(filepath) for filepath in reward_filepaths]) * 50  # make the rewards all 50 instead\n",
    "obs_arr = np.array([np.load(filepath) for filepath in obs_filepaths])\n",
    "next_obs_arr = np.array([np.load(filepath) for filepath in next_obs_filepaths])\n",
    "action_arr = np.array([np.load(filepath) for filepath in action_filepaths])\n",
    "\n",
    "# lerp the actions to the simulator range\n",
    "action_arr = lerp(action_arr, old_min=0, old_max=3400, new_min=0, new_max=1.5)\n",
    "action_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07f2dea",
   "metadata": {},
   "source": [
    "### Cleaning episode length\n",
    "\n",
    "Note: I forgot to add a `done` variable, so uhhh we're gonna just check when our finger action goes to 0 LOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9c8eb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first index of each episode where all the actions are 0.\n",
    "stop_index_arr = np.argmax(np.all(action_arr==0, axis=-1), axis=-1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25e4e36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_action = [action_arr[eps_idx, :stop_index] for eps_idx, stop_index in enumerate(stop_index_arr)]\n",
    "cut_reward = [np.concatenate((reward_arr[eps_idx, :stop_index-1], reward_arr[eps_idx, -1].reshape((1,)))) for eps_idx, stop_index in enumerate(stop_index_arr)] # set the last index to be successful based on the last timestep. We do this because we put the reward at the very end of the episode LOL\n",
    "cut_obs = [obs_arr[eps_idx, :stop_index] for eps_idx, stop_index in enumerate(stop_index_arr)]\n",
    "cut_next_obs = [next_obs_arr[eps_idx, :stop_index] for eps_idx, stop_index in enumerate(stop_index_arr)]\n",
    "\n",
    "cut_dones = [np.concatenate((np.zeros((len(eps_arr) - 1,)), np.array([1]))) for eps_arr in cut_reward]\n",
    "cut_not_dones = [np.concatenate((np.ones((len(eps_arr) - 1,)), np.array([0]))) for eps_arr in cut_reward]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bd83bc",
   "metadata": {},
   "source": [
    "### Feeding replay buffer into Queue-based Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c29dafbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================== LETS FUCKING GO\n",
      "======================== LETS FUCKING GO\n"
     ]
    }
   ],
   "source": [
    "# load the thing LOL\n",
    "expert_replay_buffer = ReplayBuffer_Queue(state_dim=len(state_idx_arr), action_dim=3, max_episode=100, n_steps=5)\n",
    "\n",
    "live_replay_buffer = ReplayBuffer_Queue(state_dim=len(state_idx_arr), action_dim=3, max_episode=100, n_steps=5)\n",
    "\n",
    "# feed the cookie monster\n",
    "\n",
    "# max_episode: Maximum number of episodes, limit to when we remove old episodes\n",
    "# size: Full size of the replay buffer (number of entries over all episodes)\n",
    "# episodes_count: Number of episodes that have occurred (may be more than max replay buffer side)\n",
    "# replay_ep_num: Number of episodes currently in the replay buffer\n",
    "\n",
    "expert_replay_buffer.store_from_replay_buffer(cut_obs, cut_action, cut_next_obs, cut_reward, cut_not_dones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df3b9b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cut_action)\n",
    "cut_action[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b091ad30",
   "metadata": {},
   "source": [
    "### Loading our pretrained policy\n",
    "\n",
    "### TODO: move constant settings for batch_size, n, discount, tau, expert sampling prop => up to the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa615eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ INITTING DDPGfD with state dim of:  26 ===============================\n",
      "ACTOR STATE DIM:  26\n"
     ]
    }
   ],
   "source": [
    "modified_state_dim = len(state_idx_arr)\n",
    "\n",
    "kwargs = {\n",
    "    \"state_dim\": modified_state_dim,\n",
    "    \"action_dim\": 3,\n",
    "    \"max_action\": 1.5,\n",
    "    \"batch_size\": 16,\n",
    "    \"n\": 5,\n",
    "    \"discount\": 0.995,\n",
    "    \"tau\": 0.0005,\n",
    "    \"expert_sampling_proportion\": 1.0\n",
    "}\n",
    "\n",
    "policy = DDPGfD(**kwargs)\n",
    "\n",
    "policy.load(trained_policy_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7c576f",
   "metadata": {},
   "source": [
    "### Feeding the replay buffer into pretrained policy (and pray that policy distribution shift doesn't fuck things up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1a34755",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'replay_buffer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d4edc0dfe614>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# fun: we need to modify so it can train with an already reduced state space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m actor_loss, critic_loss, critic_L1loss, critic_LNloss = policy.train_batch_state_already_reduced(episode_num=0, update_count=0,\n\u001b[0;32m---> 11\u001b[0;31m                                                                      \u001b[0mexpert_replay_buffer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                                                                      replay_buffer=None)\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'replay_buffer' is not defined"
     ]
    }
   ],
   "source": [
    "# NOTE: max_episode_num doesn't even get used\n",
    "# episode_num is supposed to be set by an outside loop. what the fuck\n",
    "# idk what update_count does. whatever\n",
    "\n",
    "# actor_loss, critic_loss, critic_L1loss, critic_LNloss = policy.train_batch(max_episode_num=420, episode_num=0, update_count=0,\n",
    "#                                                                      expert_replay_buffer=replay_buffer,\n",
    "#                                                                      replay_buffer=None, mod_state_idx=state_idx_arr)\n",
    "\n",
    "# fun: we need to modify so it can train with an already reduced state space\n",
    "actor_loss, critic_loss, critic_L1loss, critic_LNloss = policy.train_batch_state_already_reduced(episode_num=0, update_count=0,\n",
    "                                                                     expert_replay_buffer=expert_replay_buffer,\n",
    "                                                                     replay_buffer=None)\n",
    "\n",
    "\n",
    "# okay the bug is happening because my state dimensions don't match.\n",
    "# we can't fix this one until we're done training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e078443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training loop\n",
    "\"\"\"\n",
    "\n",
    "# set up weights and biases logging.\n",
    "import wandb\n",
    "wandb.init(project=\"kinova_grasping_irl\")\n",
    "\n",
    "wandb.watch([policy.actor, policy.critic, policy.actor_target, policy.critic_target], log_freq=1)\n",
    "\n",
    "num_batches = 100\n",
    "for batch_idx in range(num_batches):\n",
    "    actor_loss, critic_loss, critic_L1loss, critic_LNloss = policy.train_batch_state_already_reduced(episode_num=batch_idx, update_count=batch_idx,\n",
    "                                                                     expert_replay_buffer=replay_buffer,\n",
    "                                                                     replay_buffer=None)\n",
    "    \n",
    "    wandb.log({\n",
    "        'actor_loss': actor_loss,\n",
    "        'critic_loss': critic_loss,\n",
    "        'critic_L1loss': critic_L1loss,\n",
    "        'critic_LNloss': critic_LNloss\n",
    "    })\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "18e7c932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Testing block\n",
    "# \"\"\"\n",
    "# # sample from the replay buffer\n",
    "# replay_buffer.sample_batch_nstep(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c444dea",
   "metadata": {},
   "source": [
    "### Save the pretrained policy to a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78b335a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'policies/real_world_train_test_folder/real_world_trained'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_dir = 'policies'\n",
    "named_policy_folder = 'real_world_train_test_folder'\n",
    "\n",
    "policy_path = Path(os.path.join(policy_dir, named_policy_folder))\n",
    "policy_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "policy_save_basepath = os.path.join(policy_dir, named_policy_folder, 'real_world_trained')\n",
    "\n",
    "policy_save_basepath\n",
    "\n",
    "# policy.save(policy_save_basepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fe9749",
   "metadata": {},
   "source": [
    "### Load it to agent (testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72170094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import RLAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b98be3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_arr[6][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73c65741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ INITTING DDPGfD with state dim of:  26 ===============================\n",
      "ACTOR STATE DIM:  26\n",
      "tensor([[-4.7680e-02,  1.2005e-02,  2.0412e-02, -6.5911e-02,  4.6204e-02,\n",
      "          2.9962e-02,  5.4059e-02,  1.9765e-02, -2.1176e-02,  7.1118e-02,\n",
      "          4.2466e-02, -2.6887e-02,  2.3416e-02,  7.2695e-02,  4.9321e-02,\n",
      "          1.1779e+02,  1.9422e+02,  1.1606e+02,  1.8972e+02,  2.0000e-02,\n",
      "          2.0000e-02,  1.0500e-01,  9.7844e-02,  9.7844e-02,  9.3329e-02,\n",
      "          9.4852e-02]], device='cuda:0')\n",
      "weight shape:  torch.Size([400, 26])\n",
      "state:  torch.Size([1, 26])\n",
      "observation shape: (26,)\n",
      "original action: [1.5 1.5 0. ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3400., 3400.,    0.], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = RLAgent(trained_policy_path=policy_save_basepath, state_dim_config='adam_sim2real_v02')\n",
    "agent.act(obs_arr[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c30f2f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ INITTING DDPGfD with state dim of:  26 ===============================\n",
      "ACTOR STATE DIM:  26\n"
     ]
    }
   ],
   "source": [
    "agent = RLAgent(trained_policy_path=trained_policy_path, state_dim_config='adam_sim2real_v02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2302e21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.8991e-02,  1.7082e-02,  6.5915e-03,  4.4975e-02,  1.7113e-02,\n",
      "          2.5604e-02,  4.4978e-02,  1.7109e-02, -2.5605e-02, -6.7455e-02,\n",
      "          4.8855e-02,  1.0550e-02,  6.3773e-02,  4.8730e-02,  2.9115e-02,\n",
      "          6.3784e-02,  4.8720e-02, -2.9118e-02,  0.0000e+00, -3.7473e-18,\n",
      "          1.3878e-17, -1.1312e-05,  7.0033e-02, -1.3139e-02,  5.5974e-06,\n",
      "         -1.7205e-05, -3.3842e-04,  3.1880e-01,  3.1225e-01,  3.1201e-01,\n",
      "          1.5902e-01,  1.5574e-01,  1.5562e-01,  2.0371e-02,  2.0371e-02,\n",
      "          1.0546e-01,  6.5762e-02,  6.7211e-02,  7.1188e-02,  7.2514e-02,\n",
      "          6.1099e-02,  6.3075e-02,  7.4826e-02,  7.1619e-02,  7.9791e-02,\n",
      "          7.6738e-02,  6.9323e-02,  6.6104e-02,  1.8545e-01,  1.6152e-04,\n",
      "          4.9643e-02,  4.9615e-02,  4.9672e-02,  4.9643e-02,  4.9643e-02,\n",
      "          3.5655e-02,  5.0162e-02,  4.0471e-01,  5.2210e-02,  3.5205e-02,\n",
      "          4.9510e-02,  5.3010e-02,  4.8131e-02,  3.5300e-02,  4.9582e-02,\n",
      "          5.3029e-02,  4.8147e-02, -7.9633e-04,  7.9633e-04, -1.0000e+00,\n",
      "          0.0000e+00,  4.9643e-02,  1.3878e-17,  4.4646e-01,  8.6409e-02,\n",
      "          2.5020e-01,  3.0792e-01,  3.0702e-01,  1.9057e-01,  2.2431e-01,\n",
      "          2.2352e-01,  1.0000e+00]], device='cuda:0')\n",
      "weight shape:  torch.Size([400, 26])\n",
      "state:  torch.Size([1, 82])\n",
      "observation shape: (82,)\n",
      "original action: [1.4999982 1.4740603 0.0841143]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mechagodzilla/kinova_venv/lib/python3.6/site-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3399.996  , 3341.2036 ,  190.65918], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simulated episode actions\n",
    "obs_filepaths = sorted(glob.glob(os.path.join('sim_positional_test_v6_constant/episodes/', 'obs*.npy')))\n",
    "obs_arr = np.array([np.load(filepath) for filepath in obs_filepaths])\n",
    "\n",
    "agent.act(obs_arr[0][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edec2690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd99212f",
   "metadata": {},
   "source": [
    "### Interactive training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ada88c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_interactive_episodes = 100\n",
    "# we accumulate actions for a single episode, and then train on it.\n",
    "\n",
    "for episode_idx in range(num_interactive_episodes):\n",
    "    # step 1: do the episode\n",
    "    # stuff here\n",
    "    \n",
    "    \n",
    "    # step 2: add to separate buffer\n",
    "    \n",
    "    # step 3: calculate stuff\n",
    "    \n",
    "    # step 4\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
